{% extends "layout.html" %}

{% block content %}

    <!-- Starts image section -->
    <div class="container-fluid" id="backg">
      <h1>Machine Learning Forecasting</h1>
      <h2>
        Case of Study: Forecasting Gas Rate based on Drilling Data Timeseries
      </h2>
      <hr>
      <section id = "main-par">
          <figure>
              <img id="fig1" alt="Time series" src="/static/images/TS.png" style="width:40%;height: 40%;"/>
          </figure>
          <p class="stpar">
            To use Machine Learning for time series forecasting problems, one must re-framed it as supervised learning problems. 
            From a sequence to pairs of input and output sequences.
            In this exercise we will do one-step multivariate time series forecasting problems using with machine learning algorithms instead of classical statistic forecasting approach. 
            The machine learning approach will be beneficial where the tradisional forecasting method cannot solve a more complex problem.
          </p>
          <p>  
            We will use available drilling dataset and use it to forecast the production gas rate one day beyond the sample dataset. In the following section, the general approach that was used in this project are explained.    
            Note that the method can also be applied in multi-step forecasting, forecasting beyond one day and be implemented in various different field.               
          </p>
          <h2>
            Time Series vs Supervised Learning
          </h2>
          <p class="ndpar">
            A time series is a sequence of numbers that are ordered by a time index. This can be thought of as a list or column of ordered values.
            Picture of data
            Time series forecasting can be framed as a supervised learning problem. A supervised learning problem is comprised of input (X) and output (y), such that an algorithm can learn how to predict the output patterns from the input patterns.
            This re-framing of your time series data allows us to access any machine learning algorithms, i.e. linear regression, RF decision tree, Deep Learning, etc, to solve our problem.            
          </p>
          <h2>
            Sliding Window For Time Series Data
          </h2>
          <p>
            Time series data can be phrased as supervised learning.
            Given a sequence of numbers for a time series dataset, we can restructure the data to look like a supervised learning problem. We can do this by using previous time steps as input variables and use the next time step as the output variable.
            Letâ€™s make this concrete with an example. Imagine we have a time series as follows:
            <table>
              <tr>
                <th>t</th>
                <th>var1</th>
                <th>var2</th>
              </tr>
              <tr>
                <td>0</td>
                <td>1</td>
                <td>10</td>
              </tr>
              <tr>
                <td>1</td>
                <td>2</td>
                <td>11</td>
              </tr>
              <tr>
                <td>2</td>
                <td>3</td>
                <td>12</td>
              </tr>
              <tr>
                <td>3</td>
                <td>4</td>
                <td>13</td>
              </tr>
              <tr>
                <td>4</td>
                <td>5</td>
                <td>14</td>
              </tr>
              <tr>
                <td>5</td>
                <td>6</td>
                <td>15</td>
              </tr>
              <br>
            </table>
            <br>
              <p>We can restructure this time series dataset as a supervised learning problem by using the value at the previous time step to predict the value at the next time-step. Re-organizing the time series dataset this way, the data would look as follows:
            </p>
            <br>
              <table>
              <tr>
                <th>t</th>
                <th>var1(t-1)</th>
                <th>var2(t-1)</th>
                <th>var1</th>
                <th>var2</th>
              </tr>
              <tr>
                <td>0</td>
                <td>?</td>
                <td>?</td>
                <td>1</td>
                <td>10</td>
              </tr>
              <tr>
                <td>1</td>
                <td>1</td>
                <td>10</td>
                <td>2</td>
                <td>11</td>
              </tr>
              <tr>
                <td>2</td>
                <td>2</td>
                <td>11</td>
                <td>3</td>
                <td>12</td>
              </tr>
              <tr>
                <td>3</td>
                <td>3</td>
                <td>12</td>
                <td>4</td>
                <td>13</td>
              </tr>
              <tr>
                <td>4</td>
                <td>4</td>
                <td>13</td>
                <td>5</td>
                <td>14</td>
              </tr>
              <tr>
                <td>5</td>
                <td>5</td>
                <td>14</td>
                <td>6</td>
                <td>15</td>
              </tr>
              <tr>
                <td>6</td>
                <td>6</td>
                <td>15</td>
                <td>?</td>
                <td>?</td>
              </tr>
            </table>
            <br>
           <p> Here are some observations of the transformed dataset above:

           </p>
            <ul>
              <li>
                The previous time steps (t-1) are the input (X) and the next time steps (t) is the output (y) in our supervised learning problem.
              </li>
              <li>
                The order between the observations is preserved, and must continue to be preserved when using this dataset to train a supervised model. Hence the use of `train_test_split` on the Sklearn cannot be use in this approach.
              </li>
              <li>
                No previous value that we can use to predict the first value in the sequence. This row will be deleted as we cannot use it.
              </li>
              <li>
                No known next value to predict for the last value in the sequence. This value will also be deleted while training our supervised model.                
              </li>
            </ul>
            <p>The use of prior time steps to predict the next time step is called the <strong> sliding window method </strong>. For short, it may be called the window method in some literature. In statistics and time series analysis, this is called a lag or lag method.
            The number of previous time steps is called the window width or size of the lag.
            This sliding window is the basis for how we can turn any time series dataset into a supervised learning problem. From this simple example, we can notice a few things:
          </p>
            <ul>
              <li>
                We can see how this can work to turn a time series into either a regression or a classification supervised learning problem for real-valued or labeled time series values.
              </li>
              <li>
                We can see how once a time series dataset is prepared this way that any of the standard linear and nonlinear machine learning algorithms may be applied, as long as the order of the rows is preserved.
              </li>
              <li>
                We can see how the width sliding window can be increased to include more previous time steps.
              </li>
              <li>
                We can see how the sliding window approach can be used on a time series that has more than one value, or so-called multivariate time series.
              </li>
            </ul>
          </p>
          <h2>
            Walk Forward Validation
          </h2>
          <p>
            In practice, we very likely will retrain our model as new data becomes available.
            This would give the model the best opportunity to make good forecasts at each time step. We can evaluate our machine learning models under this assumption.
            There are few decisions to make:
          </p>
            <ol>
              <li>
                Minimum Number of Observations. First, we must select the minimum number of observations required to train the model. This may be thought of as the window width if a sliding window is used (see next point).
              </li>
              <li>
                Sliding or Expanding Window. Next, we need to decide whether the model will be trained on all data it has available or only on the most recent observations. This determines whether a sliding or expanding window will be used.
              </li>
            </ol>
            <p>After a sensible configuration is chosen, models can be trained and evaluated.</p>
            <ul>
              <li>
                Starting at the beginning of the time series, the minimum number of samples in the window is used to train a model.
              </li>
              <li>
                The model makes a prediction for the next time step.
              </li>
              <li>
                The prediction is stored or evaluated against the known value.
              </li>
              <li>
                The window is expanded to include the known value and the process is repeated (go to step 1.)
              </li>
            </ul>
          <p>
            Because this methodology involves moving along the time series one-time step at a time, it is often called Walk Forward Testing or Walk Forward Validation. 
            Additionally, because a sliding or expanding window is used to train a model, this method is also referred to as Rolling Window Analysis or a Rolling Forecast.
          </p>


      </section>
    </div>


{% endblock %}